<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>daoist</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="分享是一种美德">
<meta property="og:type" content="website">
<meta property="og:title" content="daoist">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="daoist">
<meta property="og:description" content="分享是一种美德">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="daoist">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="daoist" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 5.4.2"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">daoist</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-2019-05-07-写于2019五一假期之后" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2022/11/23/2019-05-07-%E5%86%99%E4%BA%8E2019%E4%BA%94%E4%B8%80%E5%81%87%E6%9C%9F%E4%B9%8B%E5%90%8E/" class="article-date">
  <time datetime="2022-11-23T06:37:52.696Z" itemprop="datePublished">2022-11-23</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2022/11/23/2019-05-07-%E5%86%99%E4%BA%8E2019%E4%BA%94%E4%B8%80%E5%81%87%E6%9C%9F%E4%B9%8B%E5%90%8E/">写于2019五一假期之后</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>且夫天地为炉兮，造化为工，阴阳为炭兮，万物为铜。</p>
<hr>
<p>周二，雨。</p>
<p>刚刚经历了从新乡到广州的将近7个小时的高铁，已经累到怀疑人生了。虽然这几天都在一直怀疑人生。</p>
<p>一切都要从五月二日早上讲起。计划五一假期出去散心，衡山，长沙三日游，被突如其来的消息给破坏了，虽然早有心里准备，但来的这么突然，还是让人猝不及防，姥姥去世了。</p>
<p>匆忙买下机票，取消了长沙的车票和住宿，开始了这几天不停的思考，毫无结果的思考。</p>
<p>我的姥姥，一个再普通不过的老太太，没有接受过什么教育，依靠着本能与遗留下来的传统，抚养了包括我妈在内的五个儿女。印象最深刻的事情，一是当年一家人都在时，回我妈娘家吃饭。姥姥家房子是个二层小楼，有个小小的院子，院子里葡萄藤遮盖了天空。房子里永远不变的沙发、电视，一个老式挂扇，还有一张小小的饭桌。一家人吃饭时，总会把饭桌移到沙发旁，好节省空间和板凳，总是小孩先吃，然后是父母辈的坐上。印象中姥姥、姥爷总是不会跟我们一起吃饭，只有等我们吃完，才端着碗，快速的把饭吃完。小时候的日子，似乎总也过不完。</p>
<p>姥姥的情商在我看来，一直不算高，换句话说，就是不清楚，用河南话来讲，一个字儿：浑，言字旁、三点水好像都可以表述。我妈妈有一次用电吹风，插座漏电，我妈插电时，被电了一下，空气开关跳闸。这时如果去安慰我妈，甚至不讲话都是挺好的，但我姥姥说，差一点把你妈电死。那个浑的态度让人印象深刻。可能也是她一个人养大五个儿女，但是却很少有人记得她的好处的原因吧。</p>
<p>这几年，我姥姥腿上骨折后，走路一直不便，于是就住养老院。每次回家和离开的第一件和最后一件事，就是去看望她。似乎最后几年，已经不再那么浑。每次都开心的让我吃她藏起来的糖果，然后就是问我对象的事情，她总是乐于看到我们这一辈带来媳妇让她看看。这是她最朴素的愿望，可惜我们几个人都不争气，一个媳妇也没有带来。她最后的几年，对我们而言竟然是个空白，还真是“有子七人，莫慰母心”了。</p>
<p>关于三次死亡</p>
<p>不知道听谁说的，一个人要经历三次死亡。第一次是肉体的死亡，死亡后，只剩下一副躯壳。第二次是入土为安，这时关于身体的痕迹已经完全从世上消失，再也见不到了。第三次是在世的人，记忆中失去了这个人，也就是被遗忘。这时候，一个人存在过的所有痕迹，都已不再存在。历史记住了寥寥数人，余者连痕迹都不曾存在过，那么如我们普通人，活着到底是为了什么？</p>
<p>人类与动物的区别</p>
<p>Shamless里老爹Frank说过一句话，人类是唯一一种知道自己难逃一死的物种。想象一下，人类究竟要如何才能以清醒的状态迎接死亡。不错，人类从来都是那么独一无二的物种，唯一一种一年四季都会发情的动物。人类依靠着动物的本能延续着生存，贪婪、欲望、性、饥饿感，人类真的复杂，既要面对自己永远无法克服的动物欲望，又拥有理智的思维，清醒的头脑，时刻提醒着自己动物的本质。想象有一天，人类能够克服种种欲望，彻底摆脱自己的动物性，是不是会灭绝呢。</p>
<p>每个人的独立性与相互理解</p>
<p>人作为一个独立的个体，拥有独立的感官与思想。每个人独一无二的经历，应该会造成人类永远的孤独。的确不可能有另外一个人，完全理解另外一个人。一句老话，存粹的客观是不存在的，你永远无法与一个色盲互相理解颜色。同样的，每个人内心中的思想，更是无法被理解。</p>
<p>生气与扬弃</p>
<p>回到家中，又学会了一个新词：生气。用来表述自己对死亡的悲哀与对亲人的思念。我在想，为什么这种难过的情绪要用生气来形容呢，可能的答案是，生老天爷的气。如曹植诗中所言：感逝者之不追，情忽忽而失度。天盖高而无阶，怀此恨其谁诉。我有满腔的愤怒，奈何天高又无阶梯，我恨你啊老天爷，我生你的气啊老天，只能发泄一下自己的悲伤与愤怒，老天爷永远不会有所改变。</p>
<p>可见传统文化中，的确有很多值得我们思考学习的地方，并不是毫无缘由的风俗，很多东西旨在让我们记住。例如那漫长又枯燥的吊孝与奠的仪式，主要是增加一个生活中的仪式感。如果还和普通日子一样，岂不是太容易忘记。这几天漫长的仪式，也是为了最后再加深一下对死者的记忆。正如结婚时各种奇怪的繁琐的流程。可能也是为了让人记住结婚的不易。不要轻易离婚，那么这次的努力岂不白费。仪式感的作用，不外乎此。所以面对旧的风俗，旧的习惯，保持一颗扬弃的心，继承好的东西，摒弃坏的东西，才能更好的发展。</p>
<p>家乡与广州</p>
<p>回到广州。突然一下子还真是无法习惯。坐在地铁里，耳根子无法清净，似乎所有人都在谈论再正常不过的琐事（与死亡相比），钱，无聊的笑声。所有人都是《秃头歌女》中那个千人一面的勃比·华特森。你居然无法分辩两个不同的人有什么区别。这个现代化的、网络化的社会，让大家都接触同样的信息，说同样的烂梗，就连幽默都是一样的，这可真是天大的幽默。我想仔细记住其中一对男女的聊天内容，最后发现是徒劳，他们说过的话，你早已不知在什么地方听过千遍万遍，但是这毫无意义的话语，都无法在记忆中停留，中国这种神奇的同质化，居然如此幽默，忍不住挂上微笑。</p>
<p>而家乡则不一样了，基本看不到年轻的男子，所有的人都在享受生活，这五天中基本每顿饭都要喝酒，所有人都在不断的吞吐着香烟，见面聊天更是一致：吃点儿，喝点儿（河南话）。见面聊聊已故之人，递上一根香烟，坐在桌上，喝上一口小酒，如此惬意的生活，不比Frank差到哪去。在这里人们只能用一个词来形容：得劲儿，巴适得很。这里人们没有金钱的烦恼，脱离了广州这让人窒息的腐臭，简直就像是世外桃源。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2022/11/23/2019-05-07-%E5%86%99%E4%BA%8E2019%E4%BA%94%E4%B8%80%E5%81%87%E6%9C%9F%E4%B9%8B%E5%90%8E/" data-id="clauebqs7000r6ovrgczp8650" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%9D%82%E6%96%87/" rel="tag">杂文</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-2019-08-16-统计学习" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2022/11/23/2019-08-16-%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/" class="article-date">
  <time datetime="2022-11-23T06:37:52.696Z" itemprop="datePublished">2022-11-23</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2022/11/23/2019-08-16-%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/">统计学习</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>在最终的分析中，所有知识皆为历史。</p>
<p>在抽象的意义下，所有科学皆为数学。</p>
<p>在理性的世界里，所有判断皆为统计。</p>
<hr>
<p>最近经常看到一个三门问题，三门问题（Monty Hall problem）亦称为蒙提霍尔问题、蒙特霍问题或蒙提霍尔悖论，大致出自美国的电视游戏节目Let’s Make a Deal。问题名字来自该节目的主持人蒙提·霍尔（Monty Hall）。参赛者会看见三扇关闭了的门，其中一扇的后面有一辆汽车，选中后面有车的那扇门可赢得该汽车，另外两扇门后面则各藏有一只山羊。当参赛者选定了一扇门，但未去开启它的时候，节目主持人开启剩下两扇门的其中一扇，露出其中一只山羊。主持人其后会问参赛者要不要换另一扇仍然关上的门。问题是：换另一扇门会否增加参赛者赢得汽车的机率？</p>
<p>面对这个问题，大部分人从常识的角度来讲，都会说换与不换应该都是一样的结果，即获得汽车的概率不会改变，为1/3。但是其实这个问题并没有这么简单，我们来用统计方法推导一下。</p>
<p>如果严格按照上述的条件，即主持人清楚地知道，自己打开的那扇门后是羊，那么选手首先选择的门后为羊的概率为2/3，之后主持人打开一扇有羊的门，选手如果选择换，则一定会选到汽车，如果选择不换，则选到车的概率为0，那么可知，如果选手第一次选羊，则最终一定会换到车，即概率为2/3。如果选手第一次选到汽车1/3，那么换门一定会选到羊概率为1/3。由此可知，选择换门可以提高获得车的概率，使其有1/3提升到2/3。</p>
<p>这是一个非常反常识的例子，人们从直觉上会认为换与不换结果都是一样的，但是其实换门会让选到车的概率增大一倍。</p>
<p>再来看一个反常识的例子：史蒂芬，30岁，美国人。史蒂芬的一位邻居这样描述他：「史蒂芬害羞且內向，总是愿意提供帮忙，但对一般大众或社会议题沒有什么参与兴趣。他的性格柔弱顺从，他渴求秩序并讲究细节。」请问史蒂芬目前最可能的职业是销售还是图书馆管理员？</p>
<p>大部分人看到这个描述，第一反应就是：史蒂芬是图书管理员。内向害羞的人怎么能成为销售呢，他一定是一个图书管理员。但实际上，在美国有一千五百万名销售员，仅有十八万名图书管理员，也就是说销售的几率是图书管理员的83倍，如果我们不考虑对其性格的描述的话。也就是说我们大多数人会自动忽略这个销售人员数量的先验知识，而只以自己获得的主观信息进行判断。</p>
<p>类似的例子还有彩票，假设我们购买彩票中奖的概率为五百万分之一，一般人就会认为我们购买彩票中奖的几率为0了，但实际上我们根据墨菲定律来看一下，每次不中奖的概率为1-1/5000000，那么如果我们买的次数足够多为n次，中奖几率就会必成<script type="math/tex">1-(1-1/5000000)^n</script>，只要n足够大，就可以获得足够大的中奖几率，只不过我们需要买足够多的彩票才可以。例如我们购买了1000000次的彩票，那么，获奖的概率将会达到18.1%，将会有相当高的中奖几率。所以说只要一个人孜孜不倦的购买彩票，最终会无限接近中奖的。换个说法，只要有足够多的人购买，那么总会有人中奖。</p>
<p>上边三个例子都说明了一个问题，人类的直觉在统计结果面前，往往显得不那么正确，通常人们是以定性而非定量的方式进行思考，也容易走进统计的误区。所以不妨想想怎么以定量的方法来进行计算，以更加客观的方法将我们遇到的问题量化为数字，复杂的问题也能够一目了然。</p>
<h3 id="从统计学到机器学习-在理性的世界里，所有判断皆为统计。"><a href="#从统计学到机器学习-在理性的世界里，所有判断皆为统计。" class="headerlink" title="从统计学到机器学习-在理性的世界里，所有判断皆为统计。"></a>从统计学到机器学习-在理性的世界里，所有判断皆为统计。</h3><p>统计学不断渗透到各个领域学科之中，其中就包括计算机科学。利用量化计算和历史数据，经过相关模型和算法的迭代，使计算机自己判断和识别计算的结果，从而达到模式识别、逻辑判断、分类聚类等能力。</p>
<p>通过无数跨学科（物理、生物、心理）的专家学者的努力，参照统计物理、神经元、人类的心理学等，历经漫长的岁月，最终建立起了人工智能这一跨领域的新兴学科。参照人类学习的过程，人的绝大部分智能是通过后天训练与学习得到的，而不是天生具有的。新生儿刚出生的时候没有视觉和听觉认知能力，在成长的过程中宝宝从外界环境不断得到信息，对大脑形成刺激，从而建立起认知的能力。要给孩子建立“苹果”、“香蕉”、“熊猫”这样的抽象概念，我们需要给他看很多苹果、香蕉的实例或者图片，并反复的告诉他这些水果的名字。经过长期训练之后，终于在孩子的大脑中形成了“苹果”、“香蕉”这些抽象概念和知识，以后他就可以将这些概念运用于眼睛看到的世界。</p>
<p>机器学习运用了类似的思路，首先从大量的标记样本中提取特征，然后利用统计学习的方法建立模型，得出相同标记的物体的统计模型，接下来就可以用这个模型来对新的图像进行识别了。当然，这种做法只是代表了机器学习中一类典型的算法，称为有监督的学习。除此之外，还有无监督学习、半监督学习、强化学习等其他类型的算法。</p>
<p>历史上出现了相当多的统计学习建模方法，如分类树、SVM、Boost、随即森林等，每种方法刚问世时都是名噪一时，都在各自擅长的领域取得了很好的成绩，直到2012年Hinton小组发明了卷积神经网络AlexNet，取得了当前所有模型中最好的性能。之所以深度学习能够一统江湖，是有其深刻的原因的。SVM、AdaBoost等所谓的浅层模型并不能很好的解决复杂的图像、语音识别问题，在这些问题上会出现严重的过拟合。举个例子，我们通常所讲的机器翻译其实就是一个浅层模型，它根据目标语言与翻译语言之间简单的单词、文字的对应关系进行翻译，并不能上升到识别语义的高度，而通常人类翻译时会先将目标语言看懂，理解其含义，再使用翻译语言将其重新描述出来。深度学习之所以如此好的性能，前Google Brain研究科学家Christopher Olah在其一篇博文中曾经详细阐述过<a target="_blank" rel="noopener" href="http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/">“Deep Learning, NLP, and Representations”</a>，她用word embeddings为例说明了多层深度模型在语言处理上的优势。</p>
<p>历史选择了深度神经网络作为解决图像、声音识别、围棋等复杂 AI 问题的方法并非偶然，神经网络在理论上有万能逼近定理（universal approximation）的保证：</p>
<blockquote>
<p>只要神经元的数量足够，激活函数满足某些数学性质，至少包含一个隐含层的前馈型神经网络可以逼近闭区间上任意连续函数到任意指定精度。即用神经网络可以模拟出任意复杂的函数。我们要识别的图像、语音数据可以看做是一个向量或者矩阵，识别算法则是这些数据到类别值的一个映射函数。</p>
</blockquote>
<p>所以现在深度学习除了能够识别较为浅层的特征（像素、纹理、轮廓）之外，还能够进一步将这些浅层特征进一步组合形成元件、组件、物体的深层次特征，也就更加符合人脑的运作模式，经过多层次的特征判断，识别物体。</p>
<h3 id="AI的局限"><a href="#AI的局限" class="headerlink" title="AI的局限"></a>AI的局限</h3><p>机器学习最受批评之处在于现在计算机仍然无法自行产生可供检测的「特征」（feature）或「假设」（hypothesis），无法具有人类的常识，致使系统效用完全取決程序设计者个人的创造力。</p>
<p>2018图灵奖获得者、美国工程院院士、Facebook人工智能研究院院长Yann Lecun在台大演讲时，也曾经讲过AI的局限：由于目前比较好的AI应用都是采用监督式学习，能够准确识别人工标示过的物体，也有些好的成果是用强化学习（Reinforcement Learning）的方式，但是强化学习需要大量地收集资料来训练模型，Yann LeCun表示，对应到现实社会中的问题，监督式学习不足以成为“真的”AI。 </p>
<p><img src="/image/Yann Lecun.jpg" alt="YannLecun与LeNet5"></p>
<p>他指出，人类的学习是建立在与事物互动的过程，许多都是人类自行体会、领悟出对事物的理解，不需要每件事都要教导，举例来说，若有个物体被前面的物体挡住，人类会知道后面的物体依然存在的事实，或是物体没有另一个物体支撑就会掉落的事实。 </p>
<p>“人脑就是推理引擎！”他说明，人类靠着观察建立内部分析模型，当人类遇到一件新的事物，就能用这些既有的模型来推测，因为生活中人类接触到大量的事物和知识，而建立了“常识”。这些常识可以带领人类做出一些程序无法达到的能力，像是人类可以只看一半的脸就能想像另外一半脸，或是可以从过去的事件推测未来等。 </p>
<p>他举例，若人类看到一张战利品放不下行李箱的图片，再看到一个句子说：”这些战利品放不下行李箱，因为它太小了。“人类能够很清楚地知道“它”指的是行李箱，人类也因为知道整个社会和世界运行的规则，当没有太多的信息时，人类可以依照因果关系自动补足空白的信息。</p>
<p>他表示，对抗训练（Adversarial Training）是可以让 AI 程序拥有自学能力的方法，他解释，对抗训练就是让两个网络相互博奕，由生成器（Generator）和判别器（Discriminator）组成，生成器随机地从训练集中挑选真实数据和干扰噪音，产生新的训练样本，判别器再用与真实数据比对的方式，判断出数据的真实性，如此一来，生成器与判别器可以交互学习自动优化预测能力，创造最佳的预测模型。</p>
<h2 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h2><ol>
<li><p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MzA5ODUxOTA5Mg==&amp;mid=2652565686&amp;idx=2&amp;sn=b1cbbac765d04390fbd2e3d01fdba8ae&amp;chksm=8b7e01edbc0988fb7a6f88dd54ba00ce17a3a0cb65ef7d8272c21a760509f6e2bca0b2d3e70e&amp;mpshare=1&amp;scene=24&amp;srcid=&amp;key=68be3ae943e49b4f55c046d46e65977a73f3eb5fa0f295a1253d6a4d5a1410db1f7ef4e1fa67da908ed061703af6a74346402ab6c78b92b1fb4415ded210828c50874739b051a9b55d4ac911c1df89b2&amp;ascene=14&amp;uin=MTY0NzA5ODg4MQ%3D%3D&amp;devicetype=Windows+7&amp;version=62060833&amp;lang=zh_CN&amp;pass_ticket=t917oPftJtYv14omNOqI3bw%2BnyXGAViJisiJzM7hgmWtrY7lfgVYdw1cAmc5ZtuI">LeCun台大演讲：AI最大缺陷是缺乏常识，无监督学习突破困境</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://ccjou.wordpress.com/2009/04/29/%E8%B2%9D%E6%B0%8F%E5%AE%9A%E7%90%86-%E9%87%8F%E5%8C%96%E6%80%9D%E8%80%83%E7%9A%84%E5%88%A9%E5%99%A8/">貝氏定理──量化思考的利器</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="http://tensorinfinity.com/paper_208.html">理解计算 从根号2到AlphaGo第八季 孤独的人们</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/">Deep Learning, NLP, and Representations</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="http://www.sohu.com/a/332260644_236505">概率的意义:随机世界与大数法则</a></p>
</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2022/11/23/2019-08-16-%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/" data-id="clauebqsb00126ovr1k7r47ly" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%9D%82%E6%96%87/" rel="tag">杂文</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-2019-07-20-HMM" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2022/11/23/2019-07-20-HMM/" class="article-date">
  <time datetime="2022-11-23T06:37:52.696Z" itemprop="datePublished">2022-11-23</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2022/11/23/2019-07-20-HMM/">HMM</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>马尔可夫链的命名来自于俄国数学家安德雷·马尔可夫，以纪念其首次定义马尔科夫链和对其收敛性质所做的研究。<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Leonard_E._Baum">Leonard E. Baum</a>在1967年首次提出了隐马尔科夫链模型的基础理论。在70年代，其第一次应用于语音识别，并在80s被大规模理解和应用于多个领域之中。隐马尔科夫链由于其独特的建模方式，与其它机器学习方法有着本质的不同。常用的算法如SVM、神经网络等，其核心为找到两种模式之间的区别，即尽可能找到boundary来区分不同的种类。而HMM则是对当前模型特征进行建模，其生成结果为当前模型的序列特征。</p>
<hr>
<h2 id="HMM介绍"><a href="#HMM介绍" class="headerlink" title="HMM介绍"></a>HMM介绍</h2><p>隐马尔可夫模型是一个双随机过程，通过一个隐式的随机过程生成一系列的状态，HMM就是对这一过程进行数学建模。</p>
<p>先来看一个例子：</p>
<p>假设吉隆坡的天气有雨rain、多云cloudy、晴sunny三种情况组成，每种天气的转移概率已知，为：</p>
<script type="math/tex; mode=display">
\left[
 \begin{matrix}
   & R   & C   & S   \\
 R & 0.4 & 0.3 & 0.3 \\
 C & 0.2 & 0.6 & 0.2 \\
 S & 0.1 & 0.1 & 0.8
  \end{matrix}
\right]</script><p>这个状态转移概率矩阵中，每一行的第一列分别代表了当前天气，后边三列则为第二天天气的概率，每行的概率之和为1。并且根据如上矩阵，可以得出一个拓扑环状概率转移图，从图中任一状态可以遍历到所有其他的状态。图中所述模型也就是最简单的马尔可夫模型。</p>
<p><img src="/image/weatherTopological.png" alt=""></p>
<p>假设今天的天气是晴天，让我们考虑以下几个问题：</p>
<ol>
<li>明天的天气会是什么？</li>
<li>后天的天气会是什么？</li>
<li>第四天的天气呢？</li>
<li>接下来的天气为“晴-晴-雨-雨-晴-云-晴”的概率会是多大？</li>
<li>连续若干天天气为雨的概率有多大？（假设连续3天）</li>
<li>出现连续晴天、多云、雨的天数平均数是多少天？</li>
</ol>
<p>让我们一个接一个来回答，这里我们用下标代表第几天，上标代表天气：</p>
<p>1.根据概率表，可以很简单的得出0.8的结论</p>
<p>2.我们根据每个情况计算所有的可能性，可得：</p>
<p>结果分别为：</p>
<script type="math/tex; mode=display">P(q_{3}=q^{rain})=\sum_{\Gamma(q_{3}=q^{rain})} P(q_{1}) \times P(q_{2}) \times P(q_{3})=1 \times 0.8 \times 0.1 +1 \times 0.1 \times 0.2 +1 \times 0.1 \times 0.4=0.14</script><script type="math/tex; mode=display">P(q_{3}=q^{cloudy})=\sum_{\Gamma(q_{3}=q^{cloudy})} P(q_{1}) \times P(q_{2}) \times P(q_{3})=1 \times 0.8 \times 0.1 +1 \times 0.1 \times 0.6 +1 \times 0.1 \times 0.3=0.17</script><script type="math/tex; mode=display">P(q_{3}=q^{sunny})=\sum_{\Gamma(q_{3}=q^{summy})} P(q_{1}) \times P(q_{2}) \times P(q_{3})=1 \times 0.8 \times 0.8 +1 \times 0.1 \times 0.2 +1 \times 0.1 \times 0.3=0.69</script><p><img src="/image/HMM1.jpg" alt=""></p>
<p>3.有了上边的结果，接下来只需要继续计算即可</p>
<script type="math/tex; mode=display">P(q_{4}=q^{rain})=0.69 \times 0.1 +0.17 \times 0.2 +0.14 \times 0.4=0.159</script><p><img src="/image/HMM2.jpg" alt=""></p>
<p>4.“晴-晴-雨-雨-晴-云-晴”序列的概率</p>
<script type="math/tex; mode=display">P(q^{sunny},q^{sunny},q^{rain},q^{rain},q^{sunny},q^{cloudy},q^{sunny})\\
=P(q^{sunny}|q^{sunny},q^{sunny}|q^{sunny},q^{rain}|q^{sunny},q^{rain}|q^{rain},q^{sunny}|q^{rain},q^{cloudy}|q^{sunny},q^{sunny}|q^{cloudy})\\
=0.8 \times 0.8 \times 0.1 \times 0.4 \times 0.3 \times 0.1 \times 0.2=1.5361 \times 10^{-4}</script><p>5.连续3天下雨的概率</p>
<script type="math/tex; mode=display">P(q^{rain},q^{rain},q^{rain},q^{not rain})={0.4}^2 \times 0.6=0.096</script><p>6.连续某种天气的平均持续天数</p>
<script type="math/tex; mode=display">\overline{d_i}=\sum_{d=1}^{\infty}d \times P_{i}(d)=\sum_{d=1}^{\infty}d \times a_{ii}^{d-1}(1-a_{ii})\\
=(1-a_{ii})\sum_{d=1}^{\infty}d \times a_{ii}^{d-1}=(1-a_{ii})\frac{\partial}{\partial a_{ii}}\sum_{d=1}^{\infty} a_{ii}^{d}\\
=(1-a_{ii})\frac{\partial}{\partial a_{ii}}(\frac{a_{ii}}{1-a_{ii}})=\frac{1}{1-a_{ii}}</script><h2 id="“隐”马尔可夫模型"><a href="#“隐”马尔可夫模型" class="headerlink" title="“隐”马尔可夫模型"></a>“隐”马尔可夫模型</h2><p>截至目前，我们只能看到一个显式的模型，观察到的状态即为我们最终需要的状态，但是显然很多实际物理模型中，我们只能观察到observation，而它们隐含的状态是无法直接观察到的，这时我们就需要对马尔可夫模型进行扩展。</p>
<p>假设有三个骰子，我们进行投掷实验，每次的结果只是“正面”或“反面”两种状态，我们无法得知具体选择了哪个骰子进行了本次实验，也就无法如同之前那样建立模型来解释。此时我们就需要HMM来进行建模和解释，HMM就是在普通的马尔可夫模型上进行一些扩展。</p>
<p>除了状态转移矩阵外，为了表示每个observation所隐含的状态以及选择到某个状态的概率，我们定义三个概率表来表示HMM。分别为：</p>
<p>A：状态转移概率矩阵；B：observation概率；<script type="math/tex">\Pi</script>：初始状态概率。H代表header，T代表tail。</p>
<script type="math/tex; mode=display">A= \left[
 \begin{matrix}
     & q^1 & q^2 & q^3 \\
 q^1 & 1/3 & 1/3 & 1/3 \\
 q^2 & 1/3 & 1/3 & 1/3 \\
 q^3 & 1/3 & 1/3 & 1/3
  \end{matrix}
\right]</script><script type="math/tex; mode=display">B=\left[
 \begin{matrix}
     & q^1 & q^2 & q^3 \\
 H & 0.5 & 0.75 & 0.25 \\
 T & 0.5 & 0.25 & 0.75
  \end{matrix}
\right]</script><script type="math/tex; mode=display">\Pi=\left[
 \begin{matrix}
     & q^1 & q^2 & q^3 \\
P(q) & 1/3 & 1/3 & 1/3
  \end{matrix}
\right]</script><p>有了这三个概率表，我们就能够回答一些相关的问题，例如已知某个observation序列，求最有可能得出此observation的状态序列，或者已知某个observation序列完全由某个状态所得的概率等。</p>
<h2 id="HMM的三个基本问题"><a href="#HMM的三个基本问题" class="headerlink" title="HMM的三个基本问题"></a>HMM的三个基本问题</h2><p>根据HMM中得出的A、B、<script type="math/tex">\Pi</script>等矩阵，可以利用viterbi、baum-welch算法计算HMM中我们感兴趣的问题，如：模式识别、序列分析、参数计算等，我们分别进行讨论。</p>
<p>在开始之前，我们首先定义一些符号，方便理解。<script type="math/tex">\lambda=(\Gamma,A,B)</script>参数能够最大化<script type="math/tex">P(X \vert \lambda)</script>；X：observation sequence；<script type="math/tex">\Gamma</script>：states sequence。</p>
<h3 id="模式识别"><a href="#模式识别" class="headerlink" title="模式识别"></a>模式识别</h3><p>给出特定的observation序列，如何找到一个HMM模型使之拥有最大似然。即计算：</p>
<script type="math/tex; mode=display">{\lambda}_{max}=\mathop{\arg\max}_{\lambda_i} P(X|{\lambda}_i) \qquad for \quad  i=1,...K</script><p>上式含义即为如何快速的找到一个HMM模型，使之对X得到最大的似然。例如手写英文字母的识别中，如何从26个HMM模型中快速计算哪个<script type="math/tex">\lambda</script>计算出此序列的值最大，那么可以判断此observation对应于具体某个字母。</p>
<p>计算这个问题最大的困难在于效率，我们先来看怎样计算<script type="math/tex">P(X\vert\lambda)</script>。X(x1,x2,…,xt)为我们最终观察到的序列，可以通过不同的状态获得相同的观察序列。</p>
<script type="math/tex; mode=display">P(X|\lambda)=\sum_{\Gamma}P(X,\Gamma|\lambda)=\sum_{\Gamma}P(X|\Gamma,\lambda) \times P(\Gamma|\lambda)</script><p>式中共有两个部分，一为<script type="math/tex">P(X\vert\Gamma,\lambda)</script>，另一部分为<script type="math/tex">P(\Gamma\vert\lambda)</script>，我们分别推导这两部分的公式，然后得出最终的时间复杂度。</p>
<p>1.<script type="math/tex">P(X\vert\Gamma,\lambda)</script></p>
<script type="math/tex; mode=display">P(X|\Gamma,\lambda)=P(x_1 x_2... x_T|q_1 q_2 ...q_T,\lambda)\\
=P(x_1|q_1...q_T,\lambda) \times P(x_2|x_1,q_1...q_T,\lambda)...P(x_T|x_{T-1}...x_1,q_1...q_T,\lambda)\\
=P(x_1|q_1,\lambda)\times P(x_2|q_2,\lambda)...P(x_T|q_T,\lambda)=\prod_{t=1}^{T}P(x_t|q_t,\lambda)</script><p>式中假设每个<script type="math/tex">x_t</script>仅仅依赖于<script type="math/tex">q_t</script></p>
<p>2.<script type="math/tex">P(\Gamma\vert\lambda)</script></p>
<script type="math/tex; mode=display">P(\Gamma|\lambda)=P(q_1q_2...q_T|\lambda)\\
=P(q_1|\lambda)\times P(q_2|q_1,\lambda)...P(q_T|q_{T-1}...q_1,\lambda)\\
=P(q_1|\lambda)\times P(q_2|q_1,\lambda)...P(q_T|q_{T-1},\lambda)\\
=P(q_1|\lambda)\times \prod_{t=2}^T P(q_t|q_{t-1},\lambda)</script><p>式中假设每个时间点的状态仅仅依赖于上一个时间点的状态</p>
<p>于是可以得到：<script type="math/tex">P(X\vert\lambda)=\sum_{\Gamma}\prod_{t=1}^{T}P({x_t}\vert{q_t},\lambda) \times P({q_1}\vert\lambda)\times \prod_{t=2}^T P({q_t}\vert{q_{t-1}},\lambda)</script>。</p>
<p>我们可以看到由于每个T都有N中可能的状态，其时间复杂度为<script type="math/tex">O(N^T)</script>，在N、T较大时，其时间复杂度是无法接受的。</p>
<p>显然这个算法重复计算了很多数据，造成了很多的浪费，具体如图所示。而使用迭代的方法，可以利用空间换取时间的效率，有效降低时间复杂度。</p>
<p>这里我们介绍一个forward algorithm，它需要3步：</p>
<ol>
<li>初始化<script type="math/tex">\alpha_1^i=P(x_1,q_1=q^i \vert \lambda)=P(x_1 \vert q_1 =q^i,\lambda)\times P(q_1=q^i \vert \lambda)</script></li>
<li>迭代<script type="math/tex">\alpha_{t+1}^j=[\sum_{i=1}^i \alpha_t^i P(q_{t+1}=q^j \vert q_t=q^i,\lambda)]\times P(x_{t+1}\vert q^j,\lambda) \\
with \quad 1\leq j \leq N,and \quad 1\leq t \leq T-1</script></li>
<li>最终可以得到<script type="math/tex">P(X \vert \lambda)=\sum_{i=1}^N \alpha_T^i</script></li>
</ol>
<p><img src="/image/HMM3.jpg" alt=""></p>
<p>根据<script type="math/tex">\alpha_{t+1}^j</script>的公式，可以得出其时间复杂度为<script type="math/tex">O(N^2T)</script>，显然<script type="math/tex">O(N^2T) \ll O(N^T)</script></p>
<p>FORWARD算法如下：</p>
<p>1、initialization</p>
<p>for <script type="math/tex">1 \leq t \leq T</script></p>
<p>&emsp; <script type="math/tex">for 1 \leq i \leq N</script></p>
<p>&emsp; &emsp; <script type="math/tex">a_t(i)=0</script></p>
<p>for <script type="math/tex">1 \leq i \leq N</script></p>
<p>&emsp; <script type="math/tex">a_1(i)=\Pi_i b_i (x_i);</script></p>
<p>2、recursive computation</p>
<p>for <script type="math/tex">2 \leq t \leq T</script></p>
<p>&emsp; <script type="math/tex">for 1 \leq i \leq N</script></p>
<p>&emsp; &emsp; <script type="math/tex">for 1 \leq j \leq N</script></p>
<p>&emsp; &emsp; &emsp; <script type="math/tex">a_t(i)=a_t(i)+a_{t-1}(j)a_{ij};</script></p>
<p>&emsp; &emsp; <script type="math/tex">a_t(i)=a_t(i)b_j(x_t);</script></p>
<p>3、termination</p>
<p>for <script type="math/tex">1 \leq i \leq N</script></p>
<p>&emsp; <script type="math/tex">P=P+a_T(i)</script></p>
<p>于是，根据FORWARD算法，我们可以得到路径中每一处的概率，同时也拥有可接受的时间复杂度。</p>
<p>显然，FORWARD算法相对应的就是BACKWARD算法，初始化时间T时的各个节点的概率为1，从后向前进行计算即可，同样可以得到任意一条路径的概率大小。</p>
<h3 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h3><p>假设我们知道了一个observation序列，那么我们如何找到最能与之匹配的状态序列呢，也就是说如何找到序列路径Q，使<script type="math/tex">P(q_1,q_2,...,q_T \vert X,\lambda)</script>取最大值，这是实际应用时最常见的问题。我们可以将其定义为如下最大后验概率问题</p>
<script type="math/tex; mode=display">\mathop{\arg\max}_{q} P(q|x)= \mathop{\arg\max}_{q} \frac{P(x,q)}{P(x)} \\
=\mathop{\arg\max}_{q} \frac{P(x,q)}{\sum_q P(x,q)} \\
=\mathop{\arg\max}_{q}P(x,q)</script><p>为了解决这个问题，我们需要引入一个新的算法：Viterbi算法，这是一个保持最可能状态序列的动态归纳算法。这相当于寻找一条产生最大概率的路径，这条路径对应着一个状态序列。这和前面的前向算法类似，只要把求和换成求最大值即可。Viterbi算法的核心是动态规划求解最优路径：要保证一个解是全局最优解，其部分解也必须是最优的。</p>
<script type="math/tex; mode=display">\delta_{t-1}(k)=max[\delta_t(i)a_{ik}]b_k(x_{t+1}),with \quad 1 \leq k \leq N</script><p>Viterbi算法如下：</p>
<p>1、initialization</p>
<p>for <script type="math/tex">1 \leq i \leq N</script></p>
<p>&emsp; <script type="math/tex">\delta_1(i)=\Pi_ib_i(x_1)</script></p>
<p>&emsp; <script type="math/tex">\Psi_1(i)=0</script></p>
<p>2、recursive computation</p>
<p>for <script type="math/tex">2 \leq y \leq T</script></p>
<p>&emsp; for <script type="math/tex">1 \leq j \leq N</script></p>
<p>&emsp; &emsp; <script type="math/tex">\delta_t(j)=max[\delta_{t-1}(i)a_{ij}]b_j(x_t)</script></p>
<p>&emsp; &emsp; <script type="math/tex">\Psi_t(j)=\mathop{\arg\max}_{1 \leq i \leq N}(\delta_{t-1}(i)a_{ij})</script></p>
<p>3、termination</p>
<p>&emsp; <script type="math/tex">P^\ast=max[\delta_T(i)]</script></p>
<p>&emsp; <script type="math/tex">q^{\ast}_{T}=\mathop{\arg\max}_{1 \leq i \leq N}[\delta_T(i)]</script></p>
<p>4、Backtracking</p>
<p>for t=T-1 down to 1    </p>
<p>&emsp; <script type="math/tex">q^{\ast}_{t}=\Psi_t(q^{\ast}_{t+1})</script></p>
<p>由于其计算过程与forward算法类似，所以计算复杂度也为<script type="math/tex">O(N^2T)</script>。</p>
<h3 id="学习"><a href="#学习" class="headerlink" title="学习"></a>学习</h3><p>顾名思义，即为从数据中找到每个模型的参数，使<script type="math/tex">\lambda=(\Gamma,A,B)</script>参数能够最大化<script type="math/tex">P(X \vert \lambda)</script>。通过一组样本，利用最大似然估计，确定状态转移矩阵和观测矩阵等模型参数。</p>
<p>为了计算这些参数，我们引入另一个算法：Baum-Welch算法，这个算法是基于EM算法得来的，其大致步骤可以简述为：</p>
<ol>
<li><p>随机初始化参数<script type="math/tex">\lambda_0</script></p>
</li>
<li><p>基于<script type="math/tex">\lambda_0</script>和observation利用EM算法计算新模型的<script type="math/tex">\lambda_0</script></p>
</li>
<li><p>如果<script type="math/tex">logP(X \vert \lambda)-logP(x \vert \lambda_0) < \delta</script>则停止计算，否则进入第四步</p>
</li>
<li><p>将<script type="math/tex">\lambda</script>赋值给<script type="math/tex">\lambda_0</script>，并重复第二步</p>
</li>
</ol>
<p>Baum-Welch算法中的EM具体推导步骤参见<a target="_blank" rel="noopener" href="http://tensorinfinity.com/paper_99.html">理解隐马尔可夫模型</a>的“训练算法”，通过EM算法构造下届函数，不断迭代逼近最优结果，最终满足停止计算的约束条件，得到<script type="math/tex">\lambda</script>。</p>
<h2 id="HMM应用"><a href="#HMM应用" class="headerlink" title="HMM应用"></a>HMM应用</h2><p>HMM被广泛应用语音识别、手写识别、DNA序列、天气、股票建模等领域。</p>
<h3 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h3><p>1、L.R.Rabiner. A tutorial on Markov models and selected applications in speech recognition, Proc. IEEE, vol. 77, pp. 257—285, Feb. 1989.</p>
<p>2、Duda, Hart and Stork, Pattern Classification, 2nd Ed., Wiley 2001.</p>
<p>3、G.A. Fink. Markov Models for Pattern Recognition, Springer, 2008.</p>
<p>4、<a target="_blank" rel="noopener" href="http://tensorinfinity.com/paper_99.html">理解隐马尔可夫模型</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2022/11/23/2019-07-20-HMM/" data-id="clauebqsb00136ovrcg4jdi35" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E7%AE%97%E6%B3%95/" rel="tag">算法</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-2019-10-13-张家界" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2022/11/23/2019-10-13-%E5%BC%A0%E5%AE%B6%E7%95%8C/" class="article-date">
  <time datetime="2022-11-23T06:37:52.696Z" itemprop="datePublished">2022-11-23</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2022/11/23/2019-10-13-%E5%BC%A0%E5%AE%B6%E7%95%8C/">张家界</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>直接上图</p>
<hr>
<p><img src="/image/张家界/张家界1.jpg" alt=""><br><img src="/image/张家界/张家界2.jpg" alt=""><br><img src="/image/张家界/张家界3.jpg" alt=""><br><img src="/image/张家界/张家界4.jpg" alt=""><br><img src="/image/张家界/张家界5.jpg" alt=""><br><img src="/image/张家界/张家界6.jpg" alt=""><br><img src="/image/张家界/张家界7.jpg" alt=""><br><img src="/image/张家界/张家界8.jpg" alt=""><br><img src="/image/张家界/张家界9.jpg" alt=""><br><img src="/image/张家界/张家界10.jpg" alt=""><br><img src="/image/张家界/张家界11.jpg" alt=""><br><img src="/image/张家界/张家界12.jpg" alt=""><br><img src="/image/张家界/张家界13.jpg" alt=""><br><img src="/image/张家界/张家界14.jpg" alt=""><br><img src="/image/张家界/张家界15.jpg" alt=""><br><img src="/image/张家界/张家界16.jpg" alt=""><br><img src="/image/张家界/张家界17.jpg" alt=""><br><img src="/image/张家界/张家界18.jpg" alt=""><br><img src="/image/张家界/张家界19.jpg" alt=""><br><img src="/image/张家界/张家界20.jpg" alt=""> </p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2022/11/23/2019-10-13-%E5%BC%A0%E5%AE%B6%E7%95%8C/" data-id="clauebqsc00156ovr5njx6uke" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%B8%B8%E8%AE%B0/" rel="tag">游记</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-2018-11-13-决策树" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2022/11/23/2018-11-13-%E5%86%B3%E7%AD%96%E6%A0%91/" class="article-date">
  <time datetime="2022-11-23T06:37:52.686Z" itemprop="datePublished">2022-11-23</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2022/11/23/2018-11-13-%E5%86%B3%E7%AD%96%E6%A0%91/">决策树</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <hr>
<p>你是否玩过二十个问题的游戏，游戏的规则很简单：参与游戏的一方在脑海里想某个事物，其他参与者向他提问题，只允许提20个问题，问题的答案也只能用对或错回答。问问题的人通过推断分析，逐步缩小待猜测事物的范围。决策树的工作原理与20个问题类似（如图1），用户输入一系列数据，然后给出游戏的答案。正方形代表判断模块，椭圆形代表终止模块，表示已经得出结论，可以终止运行。</p>
<p><img src="/image/决策树.jpg" alt="决策树示例"></p>
<p>图 1 决策树示例</p>
<p>接下来，我们引入信息论的一些概念来做为铺垫。信息论是由“信息论之父”克劳德·香农于1948年的论文《通信的数学理论》里首先提出，这里信息的定义是“facts or details that tell you something about a situation、 person、 event、 etc”。信息论作为现代通讯系统的基础，我们这里并不做深入探讨，我们只需要了解其中一个概念，信息熵。当我们不知道某事物的具体状态，却知道它有几种可能性时，显然，可能性种类愈多，不确定性愈大。而熵就是对这种不确定性的度量。在热力学第二定律中，我们知道物质世界总是自发的由有序转化为无序，即由低熵转化为高熵，这也是为什么很多分布为高斯分布的原因。</p>
<p>在信息论中，划分数据的最大原则是：将无序的数据变得更加有序。用信息论的说法，就是根据某个特征划分数据集获得的信息增益，信息增益最高的特征就是最好的选择。如何评价信息增益，其度量方式就是熵（entropy）。熵定义为信息的期望值，如果待分类的事务可能划分在多个分类之中，则符号<script type="math/tex">x_i</script>的信息定义为：</p>
<script type="math/tex; mode=display">l(x_i)=-log_2 p(x_i)</script><p>其中<script type="math/tex">p(x_i)</script>是选择给分类的概率。</p>
<p>为了计算熵，我们需要计算其期望值：</p>
<script type="math/tex; mode=display">H=-\sum_{i=1}^{n} p(x_i) log_2 p(x_i)</script><p>其中n是分类的数目。</p>
<p>有了信息熵和信息增益的概念，我们就可以方便的构建出决策树了。为了更好的说明构建一个决策树的过程，我们给出一个具体的示例。我们使用一个预测患者隐形眼镜类型的案例来进行说明。首先，我们需要理解眼科医生是如何判断患者的隐形眼镜的类型的。如果患者的眼睛缺少眼泪，我们可以判断他不适合佩戴隐形眼镜；接下来我们再根据是否散光、年龄、近视还是远视这总共4个问题，就可以判断患者是否需要佩戴隐形眼镜，以及隐形眼镜的类型、材质（软、硬）。</p>
<p><img src="/image/决策树3.jpg" alt=""> </p>
<p>图 2 隐形眼镜配戴决策树</p>
<p>对于一名医生，他可以根据自己的经验对患者进行判断，但是如果我们把它交给计算机来判断，就要给他定下一个固定的问题顺序，比方说先问年龄，在问是否近视，然后是散光、眼泪量，这时候就有一个显著的问题出现了，我们应该根据什么来判断这个顺序呢，难道只是随机的么？这显然不符合科学的精神，这时就需要我们利用之前提到的信息增益的概念了。</p>
<p>对于这个问题，它显然是带有信息的，我们如果能够让它尽量有序的排列，而不是混乱的排列，就可以更加一目了然和简单的构建一个信息分类或者说决策树。假设对于一个样本来说，眼睛干涩的人数占10%，正常的人占90%，远视的占50%，近视的占50%，我们就可以利用这两个特征来分别计算信息熵，从而得出按哪一个特征分类的熵较低，也就可以根据此特征不断的对样本进行分类。</p>
<p>首先计算眼睛是否干涩的信息熵：</p>
<script type="math/tex; mode=display">H(A)=-(0.1 \times log_2 (0.1)+0.9 \times log_2 (0.9))=0.469</script><script type="math/tex; mode=display">H(B)=-(0.5 \times log_2 (0.5)+0.5 \times log_2 (0.5))=1</script><p>可以看到，计算出来的对于眼睛干涩与否的熵要小于按照近视、远视来分类得到的熵，于是我们就可以说，按照眼睛是否干涩来进行区分的话，得到的信息增益会更大，这个特征对于我们进行数据分析时的影响也较大，我们就可以首先使用这个特征进行分类，也就是画出了决策树中的第一个分叉。接下来就根据已经分好的数据再计算其他几个特征的信息熵和信息增益，不断的一步步的向下分类，就可以得到整颗决策树了。</p>
<p>可以看到，这个原理是非常简单的，我们根据UCI数据集中的lenses数据<sup><a href="#fn_1" id="reffn_1">1</a></sup>和python<sup><a href="#fn_2" id="reffn_2">2</a></sup>对其进行了实验，最终使用了不到200行的代码，就构建出了如下的决策树：</p>
<p><img src="/image/决策树4.png" alt=""> </p>
<p>图3 最终构建的决策树</p>
<p>决策树的使用非常广泛，在数据类型为数值型、标称型和离散型数据中都可以应用。目前在专家系统中也经常使用决策树，而且决策树给出结果往往可以匹敌在当前领域具有几十年工作经验的人类专家。其优点为：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。但是它也有很多缺点：例如易产生过度拟合的数据，因此在工程实际应用中，还需要对其做更为系统的优化才能匹配具体的问题。</p>
<p>细心的读者可能已经发现了，决策树的概念和失效分析故障树的概念十分类似，我们完全可以利用决策树的概念来进行失效分析，假设我们拥有某电子元器件的失效分析数据，我们就可以利用其具体的分析数据为划分数据的特征，不断的通过决策树中的分叉来进行故障的定位，最终得到失效分析的结果。</p>
<p>参考文献</p>
<p><sup><a href="#fn_1" id="reffn_1">1</a></sup>:UCI数据集lenses数据 <a target="_blank" rel="noopener" href="https://archive.ics.uci.edu/ml/datasets/Lenses">https://archive.ics.uci.edu/ml/datasets/Lenses</a></p>
<p><sup><a href="#fn_2" id="reffn_2">2</a></sup>:Peter Harrington，《机器学习高清实战》，北京：人民邮电出版社，2013</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2022/11/23/2018-11-13-%E5%86%B3%E7%AD%96%E6%A0%91/" data-id="clauebqs2000h6ovrht4l79yd" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E7%AE%97%E6%B3%95/" rel="tag">算法</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-2017-12-27-2017年度读书报告" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2022/11/23/2017-12-27-2017%E5%B9%B4%E5%BA%A6%E8%AF%BB%E4%B9%A6%E6%8A%A5%E5%91%8A/" class="article-date">
  <time datetime="2022-11-23T06:37:52.686Z" itemprop="datePublished">2022-11-23</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2022/11/23/2017-12-27-2017%E5%B9%B4%E5%BA%A6%E8%AF%BB%E4%B9%A6%E6%8A%A5%E5%91%8A/">2017年度读书报告</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <hr>
<p>刷豆瓣首页，看到很多年度读书报告，每个人都是几十本书几十篇读后感，想起自己这一年也读了不少书，看了不少电影，听了很多音乐，寻思自己一年来做了这么多事，但是一直浑浑噩噩，从来没有总结过收获和心得，正好趁此机会，在17年即将结束之时，对自己今年看的书进行一次总结，之前的书太多，就不一一列举，之后的书，可以一年定个计划，每年来这么一次年终总结。</p>
<p>仔细想想，我今年还是看了不少书的，虽然没有做过计划，但是断断续续也看了十几本书，找出来慢慢总结一下吧。</p>
<p>先拢共列举一下有哪些书，因为看完就扔，哪些书是今年看得，也需要好好想想。《鼠疫》、《局外人》、《活着》、《暗店街》、《废都》、《白银时代》、《妻妾成群》、《古董局中局》、《许三观卖血记》、《我不是潘金莲》、《雪中悍刀行》、《马普尔小姐最后的案件》、《苏菲的世界》，其他一些小说、专业书，一时也想不起名字来，作为日后逐渐的补充吧。这样列起来，发现还是看了不少书的，只是从来没有想过每看一本，去做这么一个总结，也就逐渐落下了许多工作，待到发现时，才恍然发觉，自己读了不少书，但是从来没有过系统的总结，读完就扔，一本书的精华就这样被我浪费了。博学多识过目不忘如钱钟书先生，还是如此的勤奋和严谨，书不仅读一遍，还会读三遍四遍，做笔记的时间是读书一倍。有这样的勤奋的精神，难怪钟书先生“过目不忘”，他说：一本书，第二遍再读，总会发现第一遍时会有很多疏忽。最精彩的句子，要读几遍之后才发现。</p>
<p>而我，读书全凭一时的兴趣，读完满足了我对这本书的好奇心，也就搁在一旁，不再染指了。难怪总说自己忘性大，记不住是有原因的啊。前事休提，做人还是要往后看嘛，以后虽谈不上治学，也要尽量多记些东西，不至于开口无言，养成对网络的依赖。说到底，虽然有一颗读书的心，但是还没能克服懒，貌似读了不少书，但都是左耳朵进，右耳朵出，没甚效果，感动了自己，但是没留下任何知识，只能夸夸其谈自己藏书几何，读书不掇，真材实料是一点也没有的。</p>
<p>先说《活着》吧，仰慕已久，对其改编的电影也是久仰大名，但是每当面对这些经典，总会先暗自发愁，小时候读经典大部头留下的阴影。又因为是电子版，一眼望去没看到实际的页数，以为也是大部头，先自怵了，但是慢慢翻看，余华对福贵一生如此平铺直叙、克制、隐忍的描写，总是让人有一种发自内心的触动，因为那不仅仅是福贵的人生，它更是我们每个人的一生，虽然没有任何隐喻、感怀，但它就是做到了让你与自己的人生进行对比，那么赤裸又原始的描写，让人不由思及这活着的意义。多说无益，就把我当时看完的心境放在这里吧：虽然我一直不以活着为意义，但是芸芸众生，每个单独的个体，都是独一无二不可替代的。于是我想起了那句话：爱上丑陋。是呀，无论生活如何继续，抱着如此爱意而活着，也是需要莫大勇气的，忘掉这重复虚妄的世界，拥抱生活吧。</p>
<p>那接下来还是要说《许三观卖血记》了，余华最出名的两个短篇小说。依旧是那么直白，他的旁观者视角让人无由的对生活的主角—自己，进行了思考，没有过多的道理，就是将一个现实的生活摆在你面前，你从中看到的，不仅仅是那个时代，更是自身，抑或是最原始、最能够触动人心的对终极的探索，那是最伟大的情感，那是最伟大的思想。余华读着是真的累。</p>
<p>《我不是潘金莲》，去年上映，因为错过了电影，又因为刘震云这个名字重新唤醒了我的某些记忆，我决定先读书，再看电影。知道刘震云，也是从他之前和冯小刚的合作《一地鸡毛》开始的，知道有这么一个荒诞的人，却没有对他有过什么更深的了解。这个故事看上去如此的荒诞不经，如果再深入一些，活脱脱一个局外人，但是作者没有从这个角度来写，这个故事，还是要放在中国这么一个社会下才有意义。讽刺也有，但书中体现的更多是荒诞。李雪莲的人生，就是从那个荒诞的离婚开始讲述，每个人都关注于自身所能涉及到的层面，不同人之间仿佛真的有一个次元壁，他们无法沟通，却又如此真实，或许可以从不同维度来说，每个人都是隐喻，每个人都代表一种互相没有交集的直线。就是这样。</p>
<p>《鼠疫》《局外人》没能读到精髓，或许应该再读一遍，或许翻译之后，少了些神韵。总之只能结合加缪的思想来看，那种悲壮的、决绝的、义无反顾的精神。如果加缪能够成为最年轻的诺贝尔文学奖得主，那么村上春树应该也能获奖才对。</p>
<p>苏童是我觊觎已久的一名作家，但是《妻妾成群》放到现在，已经很难让人产生共鸣了，颂莲来到陈府，很难让人不联想到黛玉进大观园，封建的悲剧不仅仅体现在对劳动人民的压迫上，更多的，是一种末世的颓废，一种无法言说的命运感。当然，精细的刻画和对女性心理精确的把握，让这本小说名垂青史，却也让很多人难以深入其中，所谓的没有代入感就是这么个意思吧。</p>
<p>最后，要说说《苏菲的世界》，在我看来，这真的不是一本哲学入门书，更像是一本童话，一本终将变成现实的科幻小说。它披着哲学的外衣，向你讲述着世界与我们每个人的故事，略显生硬，但是道理就是这么个道理。可以把它看成是一本参考书，时常翻阅，温故知新。</p>
<p>今年的读书报告就写这么多吧，这么多字是我这段之间中写的最多的，也是断断续续很多天拼凑起来的，读书写字我总以为要一以贯之，一口气读完、写完才有那种酣畅淋漓的畅快感，这样断断续续，停留了这么久的读书报告，也是头一次了，但是显然写字是需要状态的，灵感不在，很难写出东西来，就像这次，一月一日的下午，休息了足够久的脑袋，在这么一个昏暗的办公室，写下了这些不成逻辑的文字。好像略微填充了这三天的空虚？</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2022/11/23/2017-12-27-2017%E5%B9%B4%E5%BA%A6%E8%AF%BB%E4%B9%A6%E6%8A%A5%E5%91%8A/" data-id="clauebqs3000j6ovr30bqfsw6" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%9D%82%E6%96%87/" rel="tag">杂文</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-2018-12-30-Bayes" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2022/11/23/2018-12-30-Bayes/" class="article-date">
  <time datetime="2022-11-23T06:37:52.686Z" itemprop="datePublished">2022-11-23</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2022/11/23/2018-12-30-Bayes/">Bayes</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Thomas_Bayes">托马斯·贝叶斯</a>（1701-1761），英国统计学家、哲学家、教会牧师，生前并没有发表任何使他出名的成果，直到1763年他死后两年，他的笔记才由<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Richard_Price">理查德·普莱斯</a>编辑和出版，这些笔记中有一篇他为解决“反向概率”的问题而写的文章，也就是因为这篇文章，他的名字才用以命名贝叶斯概率。但是贝叶斯概率的开拓者和普及者是<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Pierre-Simon_Laplace">拉普拉斯</a>(没错，就是那个拉普拉斯)，贝叶斯的文章只是提出了反向概率的概念，并没有深入到问题的解释，但是贝叶斯的反向概率的思想却深刻地影响了概率论的发展。</p>
<hr>
<h2 id="贝叶斯概率"><a href="#贝叶斯概率" class="headerlink" title="贝叶斯概率"></a>贝叶斯概率</h2><p>首先我们来看一个统计学中非常典型的悖论：有一个统计学家，坐上了一架飞机，这架飞机共有四个引擎，这是故事的前提。然后在飞机的飞行过程中，突然有一个引擎故障熄火，飞机的速度也降为原来的四分之三；之后又有一个引擎故障，然后速度变成了原来的二分之一；以此类推，这架飞机只剩下了最后一个引擎，这时统计学家非常高兴地告诉大家，如果这个飞机再坏一个引擎，我们就可以停留在空中了。</p>
<p>虽然常识告诉我们飞机无法停留在空中，但是根据观测和统计的结果，统计学家得出的结论其实并没有什么问题，只是他运用的方法不适合这个场景，这里经典的概率论显然会把人们引向歧途，而考虑反向概率的贝叶斯概率论可以有效的降低这种悖论的出现。首先我们明确什么事正向概率什么是反向概率，假设有一个不透明的箱子，我们事先知道里边有若干黑球、若干红球，那么在这个基础上，可以很简单的计算出摸出各种颜色球的概率，这种正向概率的计算方法在贝叶斯之前就已经被人们掌握。但是通常人们面临的问题是：不知道箱中有多少黑球、红球，我们要根据试验的结果来进行反推，得到箱中各种颜色球的数量。这也是我们面对的各种科学问题最常见的解决思路，就是只能根据观测的结果进行假设。人类面对太阳每天的升降起落，只能推测出各种模型进行假设，最初人们的假设是地心说，因为根据人们的观测，显然太阳、月亮等各种星体是围绕着地球转动的，这是最简单、最符合我们肉眼观测的假设，因此在更深入的观测和计算之前，这就是科学；随着日心说的诞生和各种更加丰富的观测手段和计算能力的诞生，人类意识到太阳才是我们宇宙的中心；但是显然这些假设随着人类观测的深入，都被一一推翻了，当然，随着人类各种星际的探索和观测手段的提升（显微镜、SEM、TEM、粒子对撞机），现在的这个宇宙模型是否还能站的住脚，也是有待商榷的。人类面对各种科学问题，最有效、最直接的方法，就是提出符合目前能够直接、间接观测到的结果的假设，然后计算这个假设的后验概率（可能性大小）。这也是为什么贝叶斯概率能够席卷统计学届的原因，因为这从哲学上符合科学的发展规律。</p>
<p>来一个小小的公式：</p>
<script type="math/tex; mode=display">P(B|A)=P(A|B) \times P(B)/P(A)=P(AB)/P(A)</script><p>相信大家对这个公式都不陌生，这样一个简单、优美的公式，背后隐含的原理却是非常深刻。再回到文章开头的那个例子，统计学家预测飞机速度的那个故事，在这里，统计学家根据观察到的现象，做出了这样一个假设，每个引擎故障，飞机速度就会降低四分之一，我们把这个假设设为h，根据前三次的观测结果，运用最大似然估计，不难得到，三次试验的结果完全符合我们的假设，OK，那么下次的故障，会以极大的概率导致飞机速度降为0。有人可能会提出异议，只进行了三次试验，还不具有说服力，样本不够充分，先不着急，我们看看贝叶斯公式是怎么解决这个问题的。那个假设仍然记为h，飞机的正常飞行并在引擎全部故障后停止在空中的概率记为G，即飞机在符合物理规律的前提下，根据贝叶斯的反向概率 <script type="math/tex">P(h \vert G)=P(G \vert h) \times P(h) / P(G)</script> ，由于飞机仍然处于我们所认识的正常的物理世界中，因此无论我们做出何种假设，<script type="math/tex">P(G)</script>大小都不会变化，因此最终我们要求的结果<script type="math/tex">P(h \vert G) \propto P(G \vert h) \times P(h)</script>，来看一下公式右边这两项代表的含义是什么，<script type="math/tex">P(G \vert h)</script>意为在我们当前的假设下飞机停止在空中的概率，由于我们已经根据最大似然估计做出了若干次试验，我们就假设这一项的大小接近于1，再看看后一项<script type="math/tex">P(h)</script>，这是什么意思呢？没错它就是说我们当前假设的先验概率的大小，由于我们的物理常识告诉我们，一个物体是没有办法停止在空中的，那么这一项的大小可以设为0，然后再看看最终的结果，就是零，即当前假设（飞机在所有引擎故障后速度降为0）在当前给出的试验结果中是不可能存在的。</p>
<p>回到刚才的<script type="math/tex">P(h \vert G) \propto P(G \vert h) \times P(h)</script>这个式子中，我们将等号后边的两项的含义抽象出来，即当前假设在我们所观察到的数据中的可能性大小likelihood（似然）和我们当前假设本身的可能性大小prior（先验概率），这两项的乘积才是我们最终求得的结果。这样来看，结果显而易见，虽然我们的假设非常符合当前所观察到的试验结果（最大似然），但是我们的假设本身存在的概率却非常小（起码在当前的物理规律中），也就是无论我们做再多次试验，拥有足够多的样本，其结果还是一样的。当然，如果我们真的做了最后一次试验，将所有飞机引擎关停，那么其结果也肯定将导致似然项的大小骤减，所以很多时候，利用贝叶斯概率，不仅可以避免悖论的出现，也能够极大的减少试验的次数，使我们的科学研究更加富有效率。最后引用拉普拉斯的一句话“概率论只不过是把常识用数学公式表达了出来”。</p>
<p>虽然这个问题看似愚蠢，但是其隐含的道理却非常深刻。人们可能会疑惑，飞机显然是不可能停在半空中的嘛，这个问题人类可以根据人类自身的常识得到，但是如果想要把这个工作交给计算机去进行学习，得出结论，却需要我们花费相当大的代价来训练计算机的数学模型，因为很多在我们人类看来常识的东西，恰恰是计算机所不具备的能力，因为根据统计学习所得出的东西，往往只是事物的表层，而越是表层的东西，往往也就越复杂多变。人类最初也是如同机器一样，只能根据最表层的观察来进行判断，但是随着人类常识的积累、观测手段的提升，将原本处于表层的东西不断进行挖掘，对事物的了解越来越深入，这才逐步形成了我们当今世界的各种理论。机器学习中的计算机就像是最原始的人类的一样，即便问题在我们看来再正常、合理不过，它也会根据最表层的现象进行统计分析和判断，难以形成人类的逻辑思维能力，这也是现在神经网络吸引人们注意的地方，它想方设法将计算机中的算法用来模拟人的大脑中的神经元，对事物进行学习，以期获得和人类一样的深层次、有逻辑判断能力的计算机，但是显然，这些工作任重而道远，起码目前看来，所谓的人工智能，仍然只是对统计规律的学习而已，要想使得计算机真正具有“智能”，似乎仍然遥不可期。</p>
<h3 id="贝叶斯实例"><a href="#贝叶斯实例" class="headerlink" title="贝叶斯实例"></a>贝叶斯实例</h3><p>让我们假设有一个死理性派，偶然间遇到一位心仪女孩，为了要证明女孩是否单身，他要完成的高难度任务是：作为一个与女孩保持距离的陌生人，在女孩毫无察觉的情况下，就可以用手头有限的信息判断出女孩的单身情况，以便在表白时避免发生尴尬。</p>
<p>首先我们召集几个朋友（人越多越好）客观的对女孩是否单身进行评价，投票得出女孩单身的概率，这一步是初始化一个概率值，我们得到概率为65.65%。</p>
<p>接下来考虑一下贝叶斯理论吧，我们需要统计的数据如下：</p>
<pre><code>单身女孩使用手机的频率大于两次和小于两次的比例

非单身女孩使用手机的频率大于两次和小于两次的比例

单身女孩结伴上自习、独自上自习的比例

非单身女孩结伴上自习、独自上自习的比例

经常结伴上自习的女孩，单身、非单身的比例

经常独自上自习的女孩，单身、非单身的比例

使用手机频率大于两次的女孩，单身、非单身的比例

使用手机频率小于两次的女孩，单身、非单身的比例
</code></pre><p>假设我们观察得出的结果是，女孩喜欢结伴上自习，而且在恋爱的女孩中，喜欢结伴自习的比例为30%，在单身女孩中喜欢结伴的比例为60%。这样我们判断女孩是否单身的计算公式是：</p>
<script type="math/tex; mode=display">P(A_i│B)=(P(B|A_i)P(A_i))/(\sum_{i=1}^{n}[P(B|A_i)P(A_i)])</script><script type="math/tex; mode=display">P(A_单│B)=(P(B|A_单)P(A_单))/(\sum_{i=1}^{n}[P(B|A_i)P(A_i)])=\\
((65.65\%×60\%)/(65.65\%×60\%+34.35\%×30\%))=79.26\%</script><p>其中：B为女孩生活中透漏的信息如结伴自习的频率，A为是否单身的概率，P(A_单│B)即表明在女孩由于喜欢结伴自习，其单身的概率提高了！</p>
<p>接下来我们继续对这个数值进行修正，考虑使用手机频率的影响，在单身女孩中，手机使用率高于1.2次/小时占其中的20%；在已经恋爱的女孩中，这一数值则是60%。对于目标女孩的观察结果是，她的手机使用率高于每小时1.2次，那么：</p>
<script type="math/tex; mode=display">P(A_单│B)=(P(B|A_单)P(A_单))/(\sum_{i=1}^{n}[P(B|A_i)P(A_i)])=\\
((79.26\%×20\%)/(79.26\%×20\%+20.74\%×60\%))=56.02\%</script><p>不幸的是，似乎女孩单身的概率又减少了。</p>
<p>经过我们的不断数值的修正，最终从一个初始值得到了女孩最接近真实的单身概率。当然，这只是统计意义上的概率，可能只需要一次女孩和其他男生牵手就可以推翻以上所有的推论，但是这里要讲的是，如何在较少的观察结果下，仅凭历史统计信息得出结果的方法。虽然本方法用在追女孩上显得异常荒谬，但是用在其他的领域却能够显示出极大的优势出来。这里只是针对广大单身男性提出一个方法，权当消遣。</p>
<h3 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h3><p>这里再简单介绍一下朴素贝叶斯的定义：由统计学知，如果每个特征需要N个样本，那么对于10个特征将需要<script type="math/tex">N^{10}</script>个样本，对于包含1000个特征的词汇表将需要<script type="math/tex">N^{1000}</script>个样本。可以看到，所需要的样本数会随着特征数目增大而迅速增长。如果特征之间相互独立，那么样本数就可以从<script type="math/tex">N^{1000}</script>减少到N×1000。所谓独立（independence）指的是统计意义上的独立，即一个特征或者单词出现的可能性与它和其他单词相邻没有关系。举个例子讲，假设单词bacon出现在unhealthy后面与出现在delicious后面的概率相同。当然，我们知道这种假设并不正确，bacon常常出现在delicious附近，而很少出现在unhealthy附近，这个假设正是朴素贝叶斯分类器中朴素（naive）一词的含义。朴素贝叶斯分类器中的另一个假设是，每个特征同等重要，其实这个假设也有问题。 如果要判断留言板的留言是否得当，那么可能不需要看完所有的1000个单词，而只需要看10～20个特征就足以做出判断了。尽管上述假设存在一些小的瑕疵，但朴素贝叶斯的实际效果却很好。常见的垃圾邮件分类，就是使用了朴素贝叶斯的方法。</p>
<p>最后，如果对贝叶斯其他的应用感兴趣，可以看看<a target="_blank" rel="noopener" href="http://mindhacks.cn/">刘未鹏</a>的这篇<a target="_blank" rel="noopener" href="http://mindhacks.cn/2008/09/21/the-magical-bayesian-method/">数学之美番外篇：平凡而又神奇的贝叶斯方法</a>，本文也是在其启发下写就的。</p>
<h2 id="屠龙的宝刀-奥卡姆剃刀"><a href="#屠龙的宝刀-奥卡姆剃刀" class="headerlink" title="屠龙的宝刀-奥卡姆剃刀"></a>屠龙的宝刀-奥卡姆剃刀</h2><p>首先介绍一下<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Occam%27s_razor">奥卡姆剃刀</a>（Occam‘s Razor），14世纪时，英格兰萨里郡，有一个叫做奥卡姆的小村庄，村里有一个方济各会教士、哲学家和神学家<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/William_of_Ockham">威廉</a>，他说了一句话，叫做“Entities are not to be multiplied without necessity”，即“如无必要，勿增实体”。奥卡姆剃刀中剃刀的含义可能是因为威廉经常使用一把剃刀而得名。这个哲学道理又被称为“law of parsimony”（简约原理），其含义为：在解决问题的过程中，往往简单的方法要好过复杂的方法，也就是说，当我们为了解决问题而提出假设时，往往应该选择假设较少的那个方法。</p>
<p>奥卡姆剃刀与贝叶斯方法有着天然的联系，在我们利用贝叶斯解决问题的时候，首先就是提出假设，然后计算各个假设的可能性大小，奥卡姆剃刀就为我们提出假设指明了方向，尽量选择假设较少的那个假设。举个例子，皇帝的新衣，当一个一丝不挂的皇帝走在大街上时，大人和小孩分别对其进行了假设：</p>
<p>大人：</p>
<ol>
<li>假设皇帝身上穿着华美的衣服</li>
<li>假设只有聪明人才能看到这件衣服</li>
<li>假设我是个蠢人，所以我看到的是一丝不挂的皇帝</li>
</ol>
<p>小孩：</p>
<ol>
<li>假设皇帝没穿衣服</li>
</ol>
<p>一目了然，小孩面对这个问题，只是提出了一个假设，就满足了所有的观测结果，而大人则需要提出三个假设，才能自圆其说，满足现在的观察。那么对这两个同时符合观测结果的假设来说，我们使用奥卡姆剃刀就可以直接选择支持小孩了，因为这个假设更加简洁，更符合奥卡姆剃刀的原理。让我们回到贝叶斯概率中来，为了解决一个问题，我们会提出若干个假设，每个假设根据统计学可知，都可能造成最终结果的正确和错误两种可能，如果一个假设没有提高我们理论的准确性，那么它的唯一作用就是增加整体理论错误的可能性，也就是说为了使得我们的假设更加具有真理性，选择较少的那个假设总是没错的，当然前提是要符合我们当前的观测结果。</p>
<p>假设有一个智者，他说自己召唤了一条看不到、摸不着、无法被观测到巨龙，那么我们利用奥卡姆剃刀，就可以轻易将巨龙斩杀了，因为一条无法被观测的巨龙，在奥卡姆剃刀看来，我们就直接假设不存在这条巨龙反而更加方便，无法被观测的东西，对于人类来讲，是不具有意义的。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>其实本文啰哩啰嗦讲了这么一大堆不着边际的话，无非是想说明两个道理，简单的东西往往是有效的，生活中往往隐藏着各种数学，数学往往也只是常识的表达。贝叶斯公式简单而优美，却隐含着我们人类认知世界的规律和常识，奥卡姆剃刀更是简单明了的指出了越简单、越正确的哲学原理。纵观机器学习史上引用次数最多的几篇文章，就可以发现，文章中的原理都异常简洁，但是却说明了非常高深的道理，使得众人纷纷引用。数学中最具影响力的十大公式，同样是简洁而优美的。简单的东西往往隐藏着更加丰富的哲学道理，遇到问题，不妨想想我们自己的生活，或许其中就隐藏了解决的方法。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2022/11/23/2018-12-30-Bayes/" data-id="clauebqs4000m6ovr6q0d40z5" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E7%AE%97%E6%B3%95/" rel="tag">算法</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-2019-03-27-PCA" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2022/11/23/2019-03-27-PCA/" class="article-date">
  <time datetime="2022-11-23T06:37:52.686Z" itemprop="datePublished">2022-11-23</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2022/11/23/2019-03-27-PCA/">PCA</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>PCA（主成分分析）是一种统计分析的过程，它通过将可能相关的一组观察值通过正交变换转换为一组线性无关的变量。这个变换的定义是让第一主成份拥有尽可能大的方差，并且使之后的成分在与之前成分正交的约束条件下拥有尽可能大的方差。最终得到的矢量是一组不相关的正交基。只看PCA的定义似乎并没有什么难以理解的地方，实际应用中，按照PCA的流程一步步计算，也不会发现太多问题，但是PCA背后的原理和意义却一直被人们所忽视。对于科学问题要刨根问底，这里我们就从矩阵、特征值、协方差矩阵一步步讲述PCA背后所隐藏的原理。</p>
<hr>
<h2 id="特征值与特征向量"><a href="#特征值与特征向量" class="headerlink" title="特征值与特征向量"></a>特征值与特征向量</h2><p>线性代数大家都不陌生，最初是人们为了解决线性方程组而提出的一种解法，随着线代的不断发展，向量、向量空间、线性变换等理论逐步构建了线代的核心理论。在开始本文之前，不妨先看一个有趣的视频<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/av6540378/">3Blue1Brown：线代的本质</a>，不知道看完这个你有什么感想？反正我是觉得自己大学线代白学了，理解了矩阵的本质后，接下来的文章就容易理解多了。</p>
<p>举个例子：</p>
<script type="math/tex; mode=display">
 \left[
 \begin{matrix}
   1 & 1 \\
   -1 & 1
  \end{matrix}
  \right] 
 \left[
 \begin{matrix}
   1 \\
   2 
  \end{matrix}
  \right]=
 \left[
 \begin{matrix}
   3 \\
   1 
  \end{matrix}
  \right]</script><p><img src="/image/矩阵相乘.PNG" alt=""> </p>
<p>在这个例子中，可以看到，向量<script type="math/tex">\left[\begin{matrix}1 \\2 \end{matrix}\right]</script>在新的坐标系中（即红色坐标里）的横纵坐标分别为1、2，通过乘以矩阵<script type="math/tex">\left[\begin{matrix}1 & 1 \\-1 & 1\end{matrix}\right]</script>，该向量变成了<script type="math/tex">\left[\begin{matrix}3 \\1 \end{matrix}\right]</script>，通过之前的视频我们知道，通过这样一个矩阵，我们将初始的二维坐标系进行了拉伸和旋转，得到了一个新的坐标系。同样的一个点，在不同坐标系内拥有不同的坐标，在新的坐标系中的坐标乘以矩阵，得到了在原坐标系中的原始坐标，尽管他们都是同一个点。</p>
<p>矩阵的本质在于线性变换，也就是说对于坐标轴的拉伸和旋转，向量通过和矩阵相乘，得到了在新的坐标系中的坐标。我们看看特征值和特征向量的定义：</p>
<script type="math/tex; mode=display">Ax=\lambda x</script><script type="math/tex; mode=display">Ax-\lambda I x=0</script><script type="math/tex; mode=display">(A-\lambda I) x=0</script><script type="math/tex; mode=display">\left| A-\lambda \right| =0</script><p>通过与之前矩阵乘法的比较，可以很简单的得出结论，特征向量就是通过矩阵相乘即坐标系转换后，仅仅发生了长度改变的向量，而特征值即为它的长度伸缩倍数。在我们上边举的例子中，会发现由于发生了坐标旋转，所有的向量都旋转离开了之前的空间，无法找到一个向量在两个坐标系中拥有同样的方向，即它不存在特征向量。通过计算，我们也可以得到，其特征值是虚数。假设我们有矩阵<script type="math/tex">\left[\begin{matrix}2 & 2 \\1 & 3\end{matrix}\right]</script>，为了计算这个矩阵的特征向量和特征值，我们可以得到<script type="math/tex">\left[\begin{matrix}2-\lambda & 2 \\1 & 3\lambda \end{matrix}\right]=0</script>，<script type="math/tex">\lambda =1</script>、<script type="math/tex">\lambda =4</script>，在这个特征值下，<script type="math/tex">\vert A-\lambda \vert =0</script> 行列式会被压缩成一条直线，维度降低为1维。</p>
<h2 id="数据降维"><a href="#数据降维" class="headerlink" title="数据降维"></a>数据降维</h2><p>理解了特征值、特征向量和矩阵的线性变换后，我们来看看如何将数据进行降维。考虑有一组二维平面的数据，每个数据点在原始坐标系中都有一个坐标。而数据降维的目的就是找到一个新的坐标系，使原来的数据点尽量落在新坐标系中的一个坐标轴上，这样做的目的是尽量保持数据中携带的信息，在降维的同时尽量减少损失。如果能够找到一个坐标系，使得所有数据的y值为0，那么就可以进行降维了，因为这样并不会丢失任何信息。</p>
<p><img src="/image/坐标投影.PNG" alt=""> </p>
<p>例如图中所示，为了使数据点的X坐标有尽量大的值，我们就让<script type="math/tex">X_{1}^{2}+X_{2}^{2}+...+X_{n}^{2}</script>取最大值。可以发现如果我们将数据都均值化为0，那么这个式子就等于方差的n倍，也就是说我们的目的就是寻找一组正交基，使得所有数据变换为这个基上的坐标后，方差最大。</p>
<p>对于二维数据来讲，找到那个使得方差最大的一组基就完事了，因为我们只有两个维度，缩减一维之后，就只剩下我们需要的那个维度了。但是对于更高的维度来说，降维的工作会变得有些复杂，首先我们肯定不能只找一组基就算了，而是要找到尽可能多的表示数据信息的并且线性无关的若干组基。我们用协方差来表示这些方向的相关性，则：<script type="math/tex">Cov(a,b)=\frac{1}{n} \sum_{i=1}^{n}a_i b_i</script>，我们仅需要找到协方差等于0的基即可，即与第一个方向正交的方向，同时又是剩下的所有方差中最大的那组基，就是我们下一步要找的基。</p>
<p>为了将两者统一表示，我们使用如下矩阵：</p>
<script type="math/tex; mode=display">
XX^T= \left(
 \begin{matrix}
   \sum_{i=1}^{n}a_{i}^{2} & \sum_{i=1}^{n}a_{i} b_{i} \\
   \sum_{i=1}^{n}a_{i} b_{i} & \sum_{i=1}^{n}b_{i}^{2}
  \end{matrix}
  \right)</script><p>在这个矩阵里，对角线元素为方差，其他元素是协方差，这两者就被统一到一个矩阵中了。由于这是一个对称矩阵，而对称矩阵拥有很多良好的性质，可以将其对角化，并且使对角线上的元素从大到小排列。具体的方法就是进行奇异值分解，推导步骤这里就不再详述了。</p>
<p>回到我们最初的目的上，我们要找的是一组正交基，使得原始矩阵经过矩阵变换后，在新的坐标系下能够保留尽可能多的信息，我们假设新的坐标系下的矩阵为Y，<script type="math/tex">Y=PX</script>，则：</p>
<script type="math/tex; mode=display">

YY^T=(PX)(PX)^T=P(XX^T)P^T=PCP^T</script><p>为了使得<script type="math/tex">YY^T</script>为对角矩阵，我们就需要找到一个矩阵P，使<script type="math/tex">PCP^T</script>成为一个对角矩阵。根据是对称矩阵的性质，可知，我们要找的矩阵P即为其特征向量组成的矩阵。也就是对协方差矩阵进行奇异值分解后，得到的正交矩阵。有了P我们就可以直接计算<script type="math/tex">PX</script>了，由于P的前n行就代表了X变换后最大的n个投影方向，因此我们也要减少到几维，就用P的前几个行组成的矩阵乘以X，即可得到最终的结果。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2022/11/23/2019-03-27-PCA/" data-id="clauebqs5000o6ovraefi5nhl" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E7%AE%97%E6%B3%95/" rel="tag">算法</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-2015-12-03-Mojette" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2022/11/23/2015-12-03-Mojette/" class="article-date">
  <time datetime="2022-11-23T06:37:52.676Z" itemprop="datePublished">2022-11-23</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2022/11/23/2015-12-03-Mojette/">Mojette</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><em>Mojette Transform</em>是由Polytech Nantes的IRCCYyN实验室的Jean pierre Guédon首先提出的一种离散的几何转换，它来源于Radon transform，是Radon transform的离散的更准确的表达。</p>
<h1 id="Radon-transform"><a href="#Radon-transform" class="headerlink" title="Radon transform"></a>Radon transform</h1><p><img src="/image/mojette1.png" alt=""></p>
<p><img src="/image/mojette2.png" alt=""></p>
<p>Radon transform 是将在一个坐标系中的几何形状以一定的角度映射到一个新的坐标上，如上图所示，在笛卡尔坐标系中的椭圆以角度θ映射到了一个新的坐标系中，其公式如下：</p>
<script type="math/tex; mode=display">proj(t,\theta)=\int\int{f(x,y)\delta(t-x \cdot \theta + y \cdot sin \theta)dxdy}</script><p>符号δ表示面积相等</p>
<h1 id="Mojette-Transform"><a href="#Mojette-Transform" class="headerlink" title="Mojette Transform"></a>Mojette Transform</h1><p>Radon是连续的，Mojette则是离散的。</p>
<p><img src="/image/mojette3.png" alt=""></p>
<p><img src="/image/mojette4.png" alt=""></p>
<p><img src="/image/mojette5.png" alt=""></p>
<p>其转换公式为:</p>
<script type="math/tex; mode=display">proj(p,q,b)=\sum{f(k,l)\Delta(b-pl+qk)}</script><p> 其中，q、p为投影的方向，k、l为坐标，b=pl-qk。 可以证明，在(p,q)方向上，共有<script type="math/tex">nbins=(H-1)\vert q \vert+(W-1) \vert p  \vert +1</script>个新的坐标值（如上图，在4*4的矩形中，在(1,0)方向上有4个新的坐标值，在(1,1)方向上有7个新的坐标值）</p>
<p> 任意大小的矩形的mojette变换的算法如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">Begin</span><br><span class="line"></span><br><span class="line"> rectangle&#x27;s height and width is H and W, the projection angle is p and q</span><br><span class="line"></span><br><span class="line"> nbins=(H-1)|q|+(W-1)|p|+1</span><br><span class="line"></span><br><span class="line"> //为了方便，我们从0开始计算的新的坐标值，即从0到nbins-1</span><br><span class="line"></span><br><span class="line"> //计算补偿值，每个坐标的投影后的新的坐标减去这个补偿值即可</span><br><span class="line"></span><br><span class="line"> HL=p(H-1)</span><br><span class="line"></span><br><span class="line"> HR=p(H-1)-q(W-1)</span><br><span class="line"></span><br><span class="line"> LR=-q(W-1)</span><br><span class="line"></span><br><span class="line"> LL=0</span><br><span class="line"></span><br><span class="line"> if HL&lt;HR then offset=HL else offset=HR</span><br><span class="line"></span><br><span class="line"> if LR&lt;offset then offset=LR</span><br><span class="line"></span><br><span class="line"> if LL&lt;offset then offset=LL</span><br><span class="line"></span><br><span class="line"> //开始计算每个坐标对应的投影的值</span><br><span class="line"></span><br><span class="line"> for k=0:W-1</span><br><span class="line"></span><br><span class="line">  for l=0:H-1</span><br><span class="line"></span><br><span class="line">   b=pl-qk</span><br><span class="line"></span><br><span class="line">   proj(b-offset)+=f(k,l)</span><br><span class="line"></span><br><span class="line">  end for</span><br><span class="line"></span><br><span class="line"> end for</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure>
<p>例：</p>
<p><img src="/image/mojette6.png" alt=""></p>
<p> 如上图，假设为5*4的矩阵，横坐标k，纵坐标l，投影方向为(2,1)，首先对四个角计算其b值，将最小的设为offset，以(0,0)这一列为例计算其投影之后的值，在这一列上，共有(0,0),(2,1),(5,3)三个坐标，其b(b=pl-qk)值都为0，但是我们是从0开始索引，所以b-offset=4即为我们新的投影的坐标，而在新投影上的值为这三个值的和。</p>
<p> 通过以上算法即可计算在任意大小的矩阵中任意投影方向的Mojette变换，但在实际应用中，反mojette变换才是最有用的，inverse-mojette将在下一篇博客中具体介绍。</p>
<p> 如果有兴趣，可以登录<a target="_blank" rel="noopener" href="http://www.mojette.net/">http://www.mojette.net/</a> 查看关于Mojette的一个小游戏，其正是inverse-mojette变换的具体实例。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2022/11/23/2015-12-03-Mojette/" data-id="clauebqru00066ovr509k6r5m" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E7%AE%97%E6%B3%95/" rel="tag">算法</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-2017-05-02-越辩愈明的道理" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2022/11/23/2017-05-02-%E8%B6%8A%E8%BE%A9%E6%84%88%E6%98%8E%E7%9A%84%E9%81%93%E7%90%86/" class="article-date">
  <time datetime="2022-11-23T06:37:52.676Z" itemprop="datePublished">2022-11-23</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2022/11/23/2017-05-02-%E8%B6%8A%E8%BE%A9%E6%84%88%E6%98%8E%E7%9A%84%E9%81%93%E7%90%86/">愈辩愈明的道理</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>公说公有理，婆说婆有理</p>
<hr>
<p>讨论一下公说公有理，婆说婆有理的问题。如果遇到这种问题，不要妄下结论，因为这时问题的正确与否是次要的，主要的是看这个问题是谁说的，所以就有了公说公有理，婆说婆有理的局面。首先先问他是否是公或婆，如果是，那么他就是对的，但是这时候如果按照真理掌握在少数人手中这一论断来看，是另一个人正确了，公与吾皆错。但如果按照少数服从多数这一论断看，我与公皆对矣。也就是说，不仅要看是谁说的，还要看人数的多少，历史上这种事件一般是以少数服从多数而结束的，像哥白尼、达尔文。再从头看，如果一个人是公，一个人是婆，那又该支持谁呢？这是一个大问题，我始终没能想明白。  </p>
<p>凡事如果问为什么，那么这个世界就太矛盾了。你问问什么中国人烧香拜佛，却又无宗教感；你问为什么做人既要国事家事天下事事事关心，却又要各人自扫门前雪，休管他人瓦上霜；为什么公说公有理，婆说婆有理；什么有少数多数这一问题。越来越多的矛盾，哲学上说的好，这个世界就是一个矛盾而又统一的世界，处处有矛盾，不论大小。没有矛盾，创造矛盾，解决矛盾，这是一个螺旋上升的过程。  </p>
<p>“信言不美，美言不信。善者不辩，辩者不善。知者不博，博者不知“，道德经最后有云。所以说啊，我的这个文章也是一样的，看似写了很多道理，但是这些道理一旦付诸文字、语言，就失去了它的真理性，只是变成了纯粹的文字，想要讲这些道理讲的明白，无异于天方夜谭，痴人说梦。面对熙熙攘攘的世间理，明白是一回事，说出来又是一回事，这玄而又玄的道理，还真的是讲不明白啊。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2022/11/23/2017-05-02-%E8%B6%8A%E8%BE%A9%E6%84%88%E6%98%8E%E7%9A%84%E9%81%93%E7%90%86/" data-id="clauebqrx000a6ovrhwe2bohb" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%9D%82%E6%96%87/" rel="tag">杂文</a></li></ul>

    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/">下一页 &amp;raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9D%82%E6%96%87/" rel="tag">杂文</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B8%B8%E8%AE%B0/" rel="tag">游记</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%AE%97%E6%B3%95/" rel="tag">算法</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签云</h3>
    <div class="widget tagcloud">
      <a href="/tags/%E6%9D%82%E6%96%87/" style="font-size: 10px;">杂文</a> <a href="/tags/%E6%B8%B8%E8%AE%B0/" style="font-size: 15px;">游记</a> <a href="/tags/%E7%AE%97%E6%B3%95/" style="font-size: 20px;">算法</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/11/">十一月 2022</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2022/11/23/2019-05-07-%E5%86%99%E4%BA%8E2019%E4%BA%94%E4%B8%80%E5%81%87%E6%9C%9F%E4%B9%8B%E5%90%8E/">写于2019五一假期之后</a>
          </li>
        
          <li>
            <a href="/2022/11/23/2019-08-16-%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/">统计学习</a>
          </li>
        
          <li>
            <a href="/2022/11/23/2019-07-20-HMM/">HMM</a>
          </li>
        
          <li>
            <a href="/2022/11/23/2019-10-13-%E5%BC%A0%E5%AE%B6%E7%95%8C/">张家界</a>
          </li>
        
          <li>
            <a href="/2022/11/23/2018-11-13-%E5%86%B3%E7%AD%96%E6%A0%91/">决策树</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2022 daoist<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
</body>
</html>